{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Wandb.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP+uMmH2SCKmJpUNfEXPxnu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Biradeep/Application-of-U-Net-Neural-Network-to-Cavitation-Phenomena/blob/master/WandbUnetTraining.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j7zuK1X6hi10",
        "outputId": "2ad03ed9-efd2-45b2-e989-24c7c7834757"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y3gfw0ejhljP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "679c35eb-ecf3-48e1-893e-41005cc5d64d"
      },
      "source": [
        "!pip install wandb --upgrade"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wandb\n",
            "  Downloading wandb-0.12.0-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[?25l\r\u001b[K     |▏                               | 10 kB 26.7 MB/s eta 0:00:01\r\u001b[K     |▍                               | 20 kB 31.4 MB/s eta 0:00:01\r\u001b[K     |▋                               | 30 kB 22.3 MB/s eta 0:00:01\r\u001b[K     |▉                               | 40 kB 17.7 MB/s eta 0:00:01\r\u001b[K     |█                               | 51 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |█▏                              | 61 kB 9.4 MB/s eta 0:00:01\r\u001b[K     |█▍                              | 71 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |█▋                              | 81 kB 10.1 MB/s eta 0:00:01\r\u001b[K     |█▉                              | 92 kB 10.3 MB/s eta 0:00:01\r\u001b[K     |██                              | 102 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |██▏                             | 112 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 122 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |██▋                             | 133 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |██▉                             | 143 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |███                             | 153 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |███▎                            | 163 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |███▍                            | 174 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |███▋                            | 184 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 194 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |████                            | 204 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |████▎                           | 215 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |████▍                           | 225 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |████▋                           | 235 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |████▉                           | 245 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |█████                           | 256 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 266 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 276 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 286 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 296 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |██████                          | 307 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 317 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 327 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 337 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 348 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |███████                         | 358 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 368 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 378 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 389 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 399 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |████████                        | 409 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 419 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 430 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 440 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 450 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 460 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 471 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 481 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 491 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 501 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 512 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 522 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 532 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 542 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 552 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 563 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 573 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 583 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 593 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 604 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 614 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 624 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 634 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 645 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 655 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 665 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 675 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 686 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 696 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 706 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 716 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 727 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 737 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 747 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 757 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 768 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 778 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 788 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 798 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 808 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 819 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 829 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 839 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 849 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 860 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 870 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 880 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 890 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 901 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 911 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 921 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 931 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 942 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 952 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 962 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 972 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 983 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 993 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 1.0 MB 8.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 1.0 MB 8.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 1.0 MB 8.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 1.0 MB 8.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 1.0 MB 8.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 1.1 MB 8.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 1.1 MB 8.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 1.1 MB 8.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 1.1 MB 8.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 1.1 MB 8.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 1.1 MB 8.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.1 MB 8.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 1.1 MB 8.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 1.1 MB 8.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 1.1 MB 8.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 1.2 MB 8.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.2 MB 8.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 1.2 MB 8.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 1.2 MB 8.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 1.2 MB 8.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 1.2 MB 8.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.2 MB 8.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 1.2 MB 8.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 1.2 MB 8.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 1.2 MB 8.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 1.3 MB 8.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.3 MB 8.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 1.3 MB 8.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 1.3 MB 8.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 1.3 MB 8.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 1.3 MB 8.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.3 MB 8.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 1.3 MB 8.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 1.3 MB 8.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 1.4 MB 8.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 1.4 MB 8.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.4 MB 8.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 1.4 MB 8.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.4 MB 8.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 1.4 MB 8.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 1.4 MB 8.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.4 MB 8.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 1.4 MB 8.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 1.4 MB 8.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 1.5 MB 8.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.5 MB 8.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.5 MB 8.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 1.5 MB 8.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 1.5 MB 8.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.5 MB 8.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 1.5 MB 8.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.5 MB 8.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.5 MB 8.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.5 MB 8.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.6 MB 8.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 1.6 MB 8.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.6 MB 8.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 1.6 MB 8.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 1.6 MB 8.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 1.6 MB 8.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.6 MB 8.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.6 MB 8.4 MB/s \n",
            "\u001b[?25hCollecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.3.1-py2.py3-none-any.whl (133 kB)\n",
            "\u001b[K     |████████████████████████████████| 133 kB 69.0 MB/s \n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading shortuuid-1.0.1-py3-none-any.whl (7.5 kB)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Collecting subprocess32>=3.5.3\n",
            "  Downloading subprocess32-3.5.4.tar.gz (97 kB)\n",
            "\u001b[K     |████████████████████████████████| 97 kB 8.7 MB/s \n",
            "\u001b[?25hCollecting configparser>=3.8.1\n",
            "  Downloading configparser-5.0.2-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.2)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n",
            "Collecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.18-py3-none-any.whl (170 kB)\n",
            "\u001b[K     |████████████████████████████████| 170 kB 60.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.0 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (3.7.4.3)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.7-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 2.5 MB/s \n",
            "\u001b[?25hCollecting smmap<5,>=3.0.1\n",
            "  Downloading smmap-4.0.0-py2.py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Building wheels for collected packages: subprocess32, pathtools\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for subprocess32: filename=subprocess32-3.5.4-py3-none-any.whl size=6502 sha256=b8261bb36131056993b59719448fe30ebc267ad464cd6bf42651c8086de20d4a\n",
            "  Stored in directory: /root/.cache/pip/wheels/50/ca/fa/8fca8d246e64f19488d07567547ddec8eb084e8c0d7a59226a\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8807 sha256=47cfb1839eafd0296e8cbe53aefc74adecad6504254c2c597f5f1b5fded23c9d\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n",
            "Successfully built subprocess32 pathtools\n",
            "Installing collected packages: smmap, gitdb, subprocess32, shortuuid, sentry-sdk, pathtools, GitPython, docker-pycreds, configparser, wandb\n",
            "Successfully installed GitPython-3.1.18 configparser-5.0.2 docker-pycreds-0.4.0 gitdb-4.0.7 pathtools-0.1.2 sentry-sdk-1.3.1 shortuuid-1.0.1 smmap-4.0.0 subprocess32-3.5.4 wandb-0.12.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "_N-AlOxjhni_",
        "outputId": "517a86fa-222b-4df3-c478-59e7b7866c12"
      },
      "source": [
        "import wandb\n",
        "\n",
        "wandb.login()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "wandb: Paste an API key from your profile and hit enter: ··········\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0aH-f6M-htwz"
      },
      "source": [
        "import sys\n",
        "sys.path.append('/usr/local/lib/python3.7/site-packages/')\n",
        "sys.path.insert(0,'/content/gdrive/MyDrive/Deep-Flow-Prediction/train')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YsYTKmYsivBU"
      },
      "source": [
        "import os, sys, random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim\n",
        "\n",
        "from DfpNet import TurbNetG, weights_init\n",
        "import dataset\n",
        "import utils"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZI7CpoHnhuSJ"
      },
      "source": [
        "config = dict(\n",
        "    batch_size=1,\n",
        "    iterations = 10000,\n",
        "    learning_rate=0.0006,\n",
        "    expo = 5,\n",
        "    architecture=\"CNN\")"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Og3Dt57hxTY"
      },
      "source": [
        "device          = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "GS34GzsJhzCN",
        "outputId": "c17cbdf1-4553-4179-a71a-c87f91e1ce23"
      },
      "source": [
        "wandb.init(project=\"unet-training\")\n",
        "cfg = wandb.config"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbiradeep\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Tracking run with wandb version 0.12.0<br/>\n",
              "                Syncing run <strong style=\"color:#cdcd00\">youthful-puddle-23</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://wandb.ai/biradeep/unet-training\" target=\"_blank\">https://wandb.ai/biradeep/unet-training</a><br/>\n",
              "                Run page: <a href=\"https://wandb.ai/biradeep/unet-training/runs/2130gss2\" target=\"_blank\">https://wandb.ai/biradeep/unet-training/runs/2130gss2</a><br/>\n",
              "                Run data is saved locally in <code>/content/wandb/run-20210817_091742-2130gss2</code><br/><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gImf5KW_h2gd"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        m.weight.data.normal_(0.0, 0.02)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        m.weight.data.normal_(1.0, 0.02)\n",
        "        m.bias.data.fill_(0)\n",
        "\n",
        "def blockUNet(in_c, out_c, name, transposed=False, bn=True, relu=True, size=4, pad=1, dropout=0.):\n",
        "    block = nn.Sequential()\n",
        "    if relu:\n",
        "        block.add_module('%s_relu' % name, nn.ReLU(inplace=True))\n",
        "    else:\n",
        "        block.add_module('%s_leakyrelu' % name, nn.LeakyReLU(0.2, inplace=True))\n",
        "    if not transposed:\n",
        "        block.add_module('%s_conv' % name, nn.Conv2d(in_c, out_c, kernel_size=size, stride=2, padding=pad, bias=True))\n",
        "    else:\n",
        "        block.add_module('%s_upsam' % name, nn.Upsample(scale_factor=2, mode='bilinear')) # Note: old default was nearest neighbor\n",
        "        # reduce kernel size by one for the upsampling (ie decoder part)\n",
        "        block.add_module('%s_tconv' % name, nn.Conv2d(in_c, out_c, kernel_size=(size-1), stride=1, padding=pad, bias=True))\n",
        "    if bn:\n",
        "        block.add_module('%s_bn' % name, nn.BatchNorm2d(out_c))\n",
        "    if dropout>0.:\n",
        "        block.add_module('%s_dropout' % name, nn.Dropout2d( dropout, inplace=True))\n",
        "    return block\n",
        "    \n",
        "# generator model\n",
        "class TurbNetG(nn.Module):\n",
        "    def __init__(self, channelExponent=6, dropout=0.):\n",
        "        super(TurbNetG, self).__init__()\n",
        "        channels = int(2 ** channelExponent + 0.5)\n",
        "\n",
        "        self.layer1 = nn.Sequential()\n",
        "        self.layer1.add_module('layer1_conv', nn.Conv2d(3, channels, 4, 2, 1, bias=True))\n",
        "\n",
        "        self.layer2 = blockUNet(channels  , channels*2, 'layer2', transposed=False, bn=True,  relu=False, dropout=dropout )\n",
        "        self.layer2b= blockUNet(channels*2, channels*2, 'layer2b',transposed=False, bn=True,  relu=False, dropout=dropout )\n",
        "        self.layer3 = blockUNet(channels*2, channels*4, 'layer3', transposed=False, bn=True,  relu=False, dropout=dropout )\n",
        "        # note the following layer also had a kernel size of 2 in the original version (cf https://arxiv.org/abs/1810.08217)\n",
        "        # it is now changed to size 4 for encoder/decoder symmetry; to reproduce the old/original results, please change it to 2\n",
        "        self.layer4 = blockUNet(channels*4, channels*8, 'layer4', transposed=False, bn=True,  relu=False, dropout=dropout ,  size=4 ) # note, size 4!\n",
        "        self.layer5 = blockUNet(channels*8, channels*8, 'layer5', transposed=False, bn=True,  relu=False, dropout=dropout , size=2,pad=0)\n",
        "        self.layer6 = blockUNet(channels*8, channels*8, 'layer6', transposed=False, bn=False, relu=False, dropout=dropout , size=2,pad=0)\n",
        "     \n",
        "        # note, kernel size is internally reduced by one now\n",
        "        self.dlayer6 = blockUNet(channels*8, channels*8, 'dlayer6', transposed=True, bn=True, relu=True, dropout=dropout , size=2,pad=0)\n",
        "        self.dlayer5 = blockUNet(channels*16,channels*8, 'dlayer5', transposed=True, bn=True, relu=True, dropout=dropout , size=2,pad=0)\n",
        "        self.dlayer4 = blockUNet(channels*16,channels*4, 'dlayer4', transposed=True, bn=True, relu=True, dropout=dropout ) \n",
        "        self.dlayer3 = blockUNet(channels*8, channels*2, 'dlayer3', transposed=True, bn=True, relu=True, dropout=dropout )\n",
        "        self.dlayer2b= blockUNet(channels*4, channels*2, 'dlayer2b',transposed=True, bn=True, relu=True, dropout=dropout )\n",
        "        self.dlayer2 = blockUNet(channels*4, channels  , 'dlayer2', transposed=True, bn=True, relu=True, dropout=dropout )\n",
        "\n",
        "        self.dlayer1 = nn.Sequential()\n",
        "        self.dlayer1.add_module('dlayer1_relu', nn.ReLU(inplace=True))\n",
        "        self.dlayer1.add_module('dlayer1_tconv', nn.ConvTranspose2d(channels*2, 3, 4, 2, 1, bias=True))\n",
        "\n",
        "    def forward(self, x):\n",
        "        out1 = self.layer1(x)\n",
        "        out2 = self.layer2(out1)\n",
        "        out2b= self.layer2b(out2)\n",
        "        out3 = self.layer3(out2b)\n",
        "        out4 = self.layer4(out3)\n",
        "        out5 = self.layer5(out4)\n",
        "        out6 = self.layer6(out5)\n",
        "        dout6 = self.dlayer6(out6)\n",
        "        dout6_out5 = torch.cat([dout6, out5], 1)\n",
        "        dout5 = self.dlayer5(dout6_out5)\n",
        "        dout5_out4 = torch.cat([dout5, out4], 1)\n",
        "        dout4 = self.dlayer4(dout5_out4)\n",
        "        dout4_out3 = torch.cat([dout4, out3], 1)\n",
        "        dout3 = self.dlayer3(dout4_out3)\n",
        "        dout3_out2b = torch.cat([dout3, out2b], 1)\n",
        "        dout2b = self.dlayer2b(dout3_out2b)\n",
        "        dout2b_out2 = torch.cat([dout2b, out2], 1)\n",
        "        dout2 = self.dlayer2(dout2b_out2)\n",
        "        dout2_out1 = torch.cat([dout2, out1], 1)\n",
        "        dout1 = self.dlayer1(dout2_out1)\n",
        "        return dout1"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bovw4W_th64m",
        "outputId": "01eb9c28-f91c-468a-80ee-a6ea669df4c5"
      },
      "source": [
        "net = TurbNetG()\n",
        "\n",
        "net.apply(weights_init)\n",
        "net.to(device)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TurbNetG(\n",
              "  (layer1): Sequential(\n",
              "    (layer1_conv): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (layer2_leakyrelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "    (layer2_conv): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "    (layer2_bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (layer2b): Sequential(\n",
              "    (layer2b_leakyrelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "    (layer2b_conv): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "    (layer2b_bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (layer3_leakyrelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "    (layer3_conv): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "    (layer3_bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (layer4_leakyrelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "    (layer4_conv): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "    (layer4_bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (layer5): Sequential(\n",
              "    (layer5_leakyrelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "    (layer5_conv): Conv2d(512, 512, kernel_size=(2, 2), stride=(2, 2))\n",
              "    (layer5_bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (layer6): Sequential(\n",
              "    (layer6_leakyrelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "    (layer6_conv): Conv2d(512, 512, kernel_size=(2, 2), stride=(2, 2))\n",
              "  )\n",
              "  (dlayer6): Sequential(\n",
              "    (dlayer6_relu): ReLU(inplace=True)\n",
              "    (dlayer6_upsam): Upsample(scale_factor=2.0, mode=bilinear)\n",
              "    (dlayer6_tconv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
              "    (dlayer6_bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (dlayer5): Sequential(\n",
              "    (dlayer5_relu): ReLU(inplace=True)\n",
              "    (dlayer5_upsam): Upsample(scale_factor=2.0, mode=bilinear)\n",
              "    (dlayer5_tconv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))\n",
              "    (dlayer5_bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (dlayer4): Sequential(\n",
              "    (dlayer4_relu): ReLU(inplace=True)\n",
              "    (dlayer4_upsam): Upsample(scale_factor=2.0, mode=bilinear)\n",
              "    (dlayer4_tconv): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (dlayer4_bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (dlayer3): Sequential(\n",
              "    (dlayer3_relu): ReLU(inplace=True)\n",
              "    (dlayer3_upsam): Upsample(scale_factor=2.0, mode=bilinear)\n",
              "    (dlayer3_tconv): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (dlayer3_bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (dlayer2b): Sequential(\n",
              "    (dlayer2b_relu): ReLU(inplace=True)\n",
              "    (dlayer2b_upsam): Upsample(scale_factor=2.0, mode=bilinear)\n",
              "    (dlayer2b_tconv): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (dlayer2b_bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (dlayer2): Sequential(\n",
              "    (dlayer2_relu): ReLU(inplace=True)\n",
              "    (dlayer2_upsam): Upsample(scale_factor=2.0, mode=bilinear)\n",
              "    (dlayer2_tconv): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (dlayer2_bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (dlayer1): Sequential(\n",
              "    (dlayer1_relu): ReLU(inplace=True)\n",
              "    (dlayer1_tconv): ConvTranspose2d(128, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vugQiq0Uh-0A"
      },
      "source": [
        "from dataset import TurbDataset\n",
        "expo = 5\n",
        "testLoader = DataLoader(dataset, batch_size=1, shuffle=False)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZqJVr3ubiAe9"
      },
      "source": [
        "n_var = 3"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WACsTqBY0FD1"
      },
      "source": [
        "import time\n",
        "from utils import relative_error"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1JrJN4PZLIN",
        "outputId": "b775db6e-05eb-427d-b97f-9fb225c0e5e6"
      },
      "source": [
        "######## Settings ########\n",
        "\n",
        "# number of training iterations\n",
        "iterations = 10000\n",
        "# batch size\n",
        "batch_size = 10\n",
        "# learning rate, generator\n",
        "lrG = 0.0006\n",
        "# decay learning rate?\n",
        "decayLr = True\n",
        "# channel exponent to control network size\n",
        "expo = 5\n",
        "# data set config\n",
        "prop=None # by default, use all from \"../data/train\"\n",
        "#prop=[1000,0.75,0,0.25] # mix data from multiple directories\n",
        "# save txt files with per epoch loss?\n",
        "saveL1 = False\n",
        "\n",
        "\n",
        "##########################\n",
        "\n",
        "prefix = \"\"\n",
        "if len(sys.argv)>1:\n",
        "    prefix = sys.argv[1]\n",
        "    print(\"Output prefix: {}\".format(prefix))\n",
        "\n",
        "dropout    = 0.      # note, the original runs from https://arxiv.org/abs/1810.08217 used slight dropout, but the effect is minimal; conv layers \"shouldn't need\" dropout, hence set to 0 here.\n",
        "doLoad     = \"\"      # optional, path to pre-trained model\n",
        "\n",
        "print(\"LR: {}\".format(lrG))\n",
        "print(\"LR decay: {}\".format(decayLr))\n",
        "print(\"Iterations: {}\".format(iterations))\n",
        "print(\"Dropout: {}\".format(dropout))\n",
        "\n",
        "##########################\n",
        "\n",
        "seed = random.randint(0, 2**32 - 1)\n",
        "print(\"Random seed: {}\".format(seed))\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "#torch.backends.cudnn.deterministic=True # warning, slower\n",
        "\n",
        "# create pytorch data object with dfp dataset\n",
        "data = TurbDataset(prop, shuffle=1)\n",
        "trainLoader = DataLoader(data, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "print(\"Training batches: {}\".format(len(trainLoader)))\n",
        "dataValidation = dataset.ValiDataset(data)\n",
        "valiLoader = DataLoader(dataValidation, batch_size=batch_size, shuffle=False, drop_last=True) \n",
        "print(\"Validation batches: {}\".format(len(valiLoader)))\n",
        "\n",
        "# setup training\n",
        "epochs = int(iterations/len(trainLoader) + 0.5)\n",
        "netG = TurbNetG(channelExponent=expo, dropout=dropout)\n",
        "print(netG) # print full net\n",
        "model_parameters = filter(lambda p: p.requires_grad, netG.parameters())\n",
        "params = sum([np.prod(p.size()) for p in model_parameters])\n",
        "print(\"Initialized TurbNet with {} trainable params \".format(params))\n",
        "\n",
        "netG.apply(weights_init)\n",
        "if len(doLoad)>0:\n",
        "    netG.load_state_dict(torch.load(doLoad))\n",
        "    print(\"Loaded model \"+doLoad)\n",
        "netG.cuda()\n",
        "\n",
        "criterionL1 = nn.L1Loss()\n",
        "criterionL1.cuda()\n",
        "\n",
        "optimizerG = optim.Adam(netG.parameters(), lr=lrG, betas=(0.5, 0.999), weight_decay=0.0)\n",
        "\n",
        "targets = Variable(torch.FloatTensor(batch_size, 3, 128, 128))\n",
        "inputs  = Variable(torch.FloatTensor(batch_size, 3, 128, 128))\n",
        "targets = targets.cuda()\n",
        "inputs  = inputs.cuda()\n",
        "\n",
        "##########################\n",
        "\n",
        "columns = ['epoch','id', 'Pressure','V_x','V_y']\n",
        "test_table= wandb.Table(columns=columns)\n",
        "start_time = time.time()\n",
        "for epoch in range(epochs):\n",
        "    print(\"Starting epoch {} / {}\".format((epoch+1),epochs))\n",
        "    \n",
        "\n",
        "    \"Training loop\"\n",
        "    netG.train()\n",
        "    L1_accum = 0.0\n",
        "\n",
        "    for i, traindata in enumerate(trainLoader, 0):\n",
        "        inputs_cpu, targets_cpu = traindata\n",
        "        targets_cpu, inputs_cpu = targets_cpu.float().cuda(), inputs_cpu.float().cuda()\n",
        "        inputs.resize_as_(inputs_cpu).copy_(inputs_cpu)\n",
        "        targets.resize_as_(targets_cpu).copy_(targets_cpu)\n",
        "\n",
        "        # compute LR decay\n",
        "        if decayLr:\n",
        "            currLr = utils.computeLR(epoch, epochs, lrG*0.1, lrG)\n",
        "            if currLr < lrG:\n",
        "                for g in optimizerG.param_groups:\n",
        "                    g['lr'] = currLr\n",
        "\n",
        "        netG.zero_grad()\n",
        "        gen_out = netG(inputs)\n",
        "\n",
        "        lossL1 = criterionL1(gen_out, targets)\n",
        "        lossL1.backward()\n",
        "\n",
        "        optimizerG.step()\n",
        "\n",
        "        lossL1viz = lossL1.item()\n",
        "        L1_accum += lossL1viz\n",
        "        \n",
        "\n",
        "        if i==len(trainLoader)-1:\n",
        "            logline = \"Epoch: {}, batch-idx: {}, L1: {}\\n\".format(epoch, i, lossL1viz)\n",
        "            print(logline)\n",
        "\n",
        "    wandb.log({'Train_loss':L1_accum/len(trainLoader), 'Epoch':epoch})\n",
        "\n",
        "    # validation\n",
        "    netG.eval()\n",
        "    L1val_accum = 0.0\n",
        "    test_loss_accum = 0\n",
        "\n",
        "    dataset = TurbDataset(None, mode=TurbDataset.TEST, dataDirTest= r\"/content/gdrive/MyDrive/data/test\")\n",
        "    testLoader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "    # data for graph plotting\n",
        "    L1_accum    /= len(trainLoader)\n",
        "    L1val_accum /= len(valiLoader)\n",
        "    if saveL1:\n",
        "        if epoch==0: \n",
        "            utils.resetLog(prefix + \"L1.txt\"   )\n",
        "            utils.resetLog(prefix + \"L1val.txt\")\n",
        "        utils.log(prefix + \"L1.txt\"   , \"{} \".format(L1_accum), False)\n",
        "        utils.log(prefix + \"L1val.txt\", \"{} \".format(L1val_accum), False)\n",
        "\n",
        "    '''Relative error per channel'''\n",
        "    R_Error         = torch.zeros(size=(1,n_var),device=device)\n",
        "\n",
        "    counter = 0\n",
        "    for i, data in enumerate(testLoader, 0):\n",
        "\n",
        "      counter +=1\n",
        "      inputs_cpu, targets_cpu = data\n",
        "      targets_cpu1, inputs_cpu = targets_cpu.float().cuda(), inputs_cpu.float().cuda()\n",
        "      inputs.resize_as_(inputs_cpu).copy_(inputs_cpu)\n",
        "      targets.resize_as_(targets_cpu1).copy_(targets_cpu)     \n",
        "      #inputs.data.resize_as_(inputs_cpu).copy_(inputs_cpu)\n",
        "      #targets.data.resize_as_(targets_cpu1).copy_(targets_cpu)\n",
        "\n",
        "\n",
        "      outputs = netG(inputs)\n",
        "      outputs_cpu = outputs.data.cpu().numpy()[0]\n",
        "      targets_cpu = targets_cpu1.cpu().numpy()[0] \n",
        "\n",
        "      outputs_gpu_test = outputs.data\n",
        "      targets_gpu_test = targets_cpu1   \n",
        "\n",
        "      loss             = criterionL1(outputs,targets)\n",
        "      test_loss_accum += loss.item()\n",
        "      R_Error         += relative_error(outputs_gpu_test,targets_gpu_test, inputs_cpu[:,2,:,:])\n",
        "\n",
        "    if counter % 30==0:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      images = []\n",
        "\n",
        "      for i in range(n_var):\n",
        "        x        = np.reshape(targets_cpu[i,:,:],(128,128)).T\n",
        "        y        = np.reshape(outputs_cpu[i,:,:],(128,128)).T\n",
        "        img_data = wandb.Image(np.concatenate((x,y),axis=1))\n",
        "        images.append(img_data)\n",
        "\n",
        "      test_table.add_data(epoch,counter,*images)\n",
        "\n",
        "    wandb.log({'Channels':test_table})\n",
        "  \n",
        "\n",
        "    E_P, E_V_x, E_V_y  = R_Error[0]/len(testLoader)\n",
        "\n",
        "    wandb.log({'Test_loss':test_loss_accum/len(testLoader),'Epoch':epoch,\\\n",
        "             'E_P':E_P,'E_V_x':E_V_x,'E_V_y':E_V_y})\n",
        "  \n",
        "\n",
        "end_time = time.time()\n",
        "total_time = end_time - start_time\n",
        "print(\"Time: \", total_time)\n",
        "\n",
        "torch.save(netG.state_dict(), prefix + \"ModelGNew6.pth\" )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Output prefix: -f\n",
            "LR: 0.0006\n",
            "LR decay: True\n",
            "Iterations: 10000\n",
            "Dropout: 0.0\n",
            "Random seed: 3190650814\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}