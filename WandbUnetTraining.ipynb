{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Wandb.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMR7Kp2NvVKbA0m+VWXGUoC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Biradeep/Application-of-U-Net-Neural-Network-to-Cavitation-Phenomena/blob/master/WandbUnetTraining.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j7zuK1X6hi10",
        "outputId": "28e36a72-b1ca-43a9-989d-8a2c9f8f2faf"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y3gfw0ejhljP"
      },
      "source": [
        "!pip install wandb --upgrade"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_N-AlOxjhni_",
        "outputId": "bb5e4931-ec5b-4452-b918-0bb6ce895078"
      },
      "source": [
        "import wandb\n",
        "\n",
        "wandb.login()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbiradeep\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0aH-f6M-htwz"
      },
      "source": [
        "import sys\n",
        "sys.path.append('/usr/local/lib/python3.7/site-packages/')\n",
        "sys.path.insert(0,'/content/gdrive/MyDrive/Deep-Flow-Prediction/train')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YsYTKmYsivBU"
      },
      "source": [
        "import os, sys, random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim\n",
        "\n",
        "from DfpNet import TurbNetG, weights_init\n",
        "import dataset\n",
        "import utils"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZI7CpoHnhuSJ"
      },
      "source": [
        "config = dict(\n",
        "    batch_size=1,\n",
        "    iterations = 10000,\n",
        "    learning_rate=0.0006,\n",
        "    expo = 5,\n",
        "    architecture=\"CNN\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Og3Dt57hxTY"
      },
      "source": [
        "device          = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "id": "GS34GzsJhzCN",
        "outputId": "cac458a6-ff93-4b3a-e486-abbf39ccba4a"
      },
      "source": [
        "wandb.init(project=\"unet-training\")\n",
        "cfg = wandb.config"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Tracking run with wandb version 0.12.0<br/>\n",
              "                Syncing run <strong style=\"color:#cdcd00\">hopeful-darkness-21</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://wandb.ai/biradeep/unet-training\" target=\"_blank\">https://wandb.ai/biradeep/unet-training</a><br/>\n",
              "                Run page: <a href=\"https://wandb.ai/biradeep/unet-training/runs/6m6sinsd\" target=\"_blank\">https://wandb.ai/biradeep/unet-training/runs/6m6sinsd</a><br/>\n",
              "                Run data is saved locally in <code>/content/wandb/run-20210816_155416-6m6sinsd</code><br/><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gImf5KW_h2gd"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        m.weight.data.normal_(0.0, 0.02)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        m.weight.data.normal_(1.0, 0.02)\n",
        "        m.bias.data.fill_(0)\n",
        "\n",
        "def blockUNet(in_c, out_c, name, transposed=False, bn=True, relu=True, size=4, pad=1, dropout=0.):\n",
        "    block = nn.Sequential()\n",
        "    if relu:\n",
        "        block.add_module('%s_relu' % name, nn.ReLU(inplace=True))\n",
        "    else:\n",
        "        block.add_module('%s_leakyrelu' % name, nn.LeakyReLU(0.2, inplace=True))\n",
        "    if not transposed:\n",
        "        block.add_module('%s_conv' % name, nn.Conv2d(in_c, out_c, kernel_size=size, stride=2, padding=pad, bias=True))\n",
        "    else:\n",
        "        block.add_module('%s_upsam' % name, nn.Upsample(scale_factor=2, mode='bilinear')) # Note: old default was nearest neighbor\n",
        "        # reduce kernel size by one for the upsampling (ie decoder part)\n",
        "        block.add_module('%s_tconv' % name, nn.Conv2d(in_c, out_c, kernel_size=(size-1), stride=1, padding=pad, bias=True))\n",
        "    if bn:\n",
        "        block.add_module('%s_bn' % name, nn.BatchNorm2d(out_c))\n",
        "    if dropout>0.:\n",
        "        block.add_module('%s_dropout' % name, nn.Dropout2d( dropout, inplace=True))\n",
        "    return block\n",
        "    \n",
        "# generator model\n",
        "class TurbNetG(nn.Module):\n",
        "    def __init__(self, channelExponent=6, dropout=0.):\n",
        "        super(TurbNetG, self).__init__()\n",
        "        channels = int(2 ** channelExponent + 0.5)\n",
        "\n",
        "        self.layer1 = nn.Sequential()\n",
        "        self.layer1.add_module('layer1_conv', nn.Conv2d(3, channels, 4, 2, 1, bias=True))\n",
        "\n",
        "        self.layer2 = blockUNet(channels  , channels*2, 'layer2', transposed=False, bn=True,  relu=False, dropout=dropout )\n",
        "        self.layer2b= blockUNet(channels*2, channels*2, 'layer2b',transposed=False, bn=True,  relu=False, dropout=dropout )\n",
        "        self.layer3 = blockUNet(channels*2, channels*4, 'layer3', transposed=False, bn=True,  relu=False, dropout=dropout )\n",
        "        # note the following layer also had a kernel size of 2 in the original version (cf https://arxiv.org/abs/1810.08217)\n",
        "        # it is now changed to size 4 for encoder/decoder symmetry; to reproduce the old/original results, please change it to 2\n",
        "        self.layer4 = blockUNet(channels*4, channels*8, 'layer4', transposed=False, bn=True,  relu=False, dropout=dropout ,  size=4 ) # note, size 4!\n",
        "        self.layer5 = blockUNet(channels*8, channels*8, 'layer5', transposed=False, bn=True,  relu=False, dropout=dropout , size=2,pad=0)\n",
        "        self.layer6 = blockUNet(channels*8, channels*8, 'layer6', transposed=False, bn=False, relu=False, dropout=dropout , size=2,pad=0)\n",
        "     \n",
        "        # note, kernel size is internally reduced by one now\n",
        "        self.dlayer6 = blockUNet(channels*8, channels*8, 'dlayer6', transposed=True, bn=True, relu=True, dropout=dropout , size=2,pad=0)\n",
        "        self.dlayer5 = blockUNet(channels*16,channels*8, 'dlayer5', transposed=True, bn=True, relu=True, dropout=dropout , size=2,pad=0)\n",
        "        self.dlayer4 = blockUNet(channels*16,channels*4, 'dlayer4', transposed=True, bn=True, relu=True, dropout=dropout ) \n",
        "        self.dlayer3 = blockUNet(channels*8, channels*2, 'dlayer3', transposed=True, bn=True, relu=True, dropout=dropout )\n",
        "        self.dlayer2b= blockUNet(channels*4, channels*2, 'dlayer2b',transposed=True, bn=True, relu=True, dropout=dropout )\n",
        "        self.dlayer2 = blockUNet(channels*4, channels  , 'dlayer2', transposed=True, bn=True, relu=True, dropout=dropout )\n",
        "\n",
        "        self.dlayer1 = nn.Sequential()\n",
        "        self.dlayer1.add_module('dlayer1_relu', nn.ReLU(inplace=True))\n",
        "        self.dlayer1.add_module('dlayer1_tconv', nn.ConvTranspose2d(channels*2, 3, 4, 2, 1, bias=True))\n",
        "\n",
        "    def forward(self, x):\n",
        "        out1 = self.layer1(x)\n",
        "        out2 = self.layer2(out1)\n",
        "        out2b= self.layer2b(out2)\n",
        "        out3 = self.layer3(out2b)\n",
        "        out4 = self.layer4(out3)\n",
        "        out5 = self.layer5(out4)\n",
        "        out6 = self.layer6(out5)\n",
        "        dout6 = self.dlayer6(out6)\n",
        "        dout6_out5 = torch.cat([dout6, out5], 1)\n",
        "        dout5 = self.dlayer5(dout6_out5)\n",
        "        dout5_out4 = torch.cat([dout5, out4], 1)\n",
        "        dout4 = self.dlayer4(dout5_out4)\n",
        "        dout4_out3 = torch.cat([dout4, out3], 1)\n",
        "        dout3 = self.dlayer3(dout4_out3)\n",
        "        dout3_out2b = torch.cat([dout3, out2b], 1)\n",
        "        dout2b = self.dlayer2b(dout3_out2b)\n",
        "        dout2b_out2 = torch.cat([dout2b, out2], 1)\n",
        "        dout2 = self.dlayer2(dout2b_out2)\n",
        "        dout2_out1 = torch.cat([dout2, out1], 1)\n",
        "        dout1 = self.dlayer1(dout2_out1)\n",
        "        return dout1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bovw4W_th64m"
      },
      "source": [
        "net = TurbNetG()\n",
        "\n",
        "net.apply(weights_init)\n",
        "net.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vugQiq0Uh-0A"
      },
      "source": [
        "from dataset import TurbDataset\n",
        "expo = 5\n",
        "testLoader = DataLoader(dataset, batch_size=1, shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZqJVr3ubiAe9"
      },
      "source": [
        "n_var = 3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WACsTqBY0FD1"
      },
      "source": [
        "import time\n",
        "from utils import relative_error"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1JrJN4PZLIN"
      },
      "source": [
        "######## Settings ########\n",
        "\n",
        "# number of training iterations\n",
        "iterations = 10000\n",
        "# batch size\n",
        "batch_size = 10\n",
        "# learning rate, generator\n",
        "lrG = 0.0006\n",
        "# decay learning rate?\n",
        "decayLr = True\n",
        "# channel exponent to control network size\n",
        "expo = 5\n",
        "# data set config\n",
        "prop=None # by default, use all from \"../data/train\"\n",
        "#prop=[1000,0.75,0,0.25] # mix data from multiple directories\n",
        "# save txt files with per epoch loss?\n",
        "saveL1 = False\n",
        "\n",
        "\n",
        "##########################\n",
        "\n",
        "prefix = \"\"\n",
        "if len(sys.argv)>1:\n",
        "    prefix = sys.argv[1]\n",
        "    print(\"Output prefix: {}\".format(prefix))\n",
        "\n",
        "dropout    = 0.      # note, the original runs from https://arxiv.org/abs/1810.08217 used slight dropout, but the effect is minimal; conv layers \"shouldn't need\" dropout, hence set to 0 here.\n",
        "doLoad     = \"\"      # optional, path to pre-trained model\n",
        "\n",
        "print(\"LR: {}\".format(lrG))\n",
        "print(\"LR decay: {}\".format(decayLr))\n",
        "print(\"Iterations: {}\".format(iterations))\n",
        "print(\"Dropout: {}\".format(dropout))\n",
        "\n",
        "##########################\n",
        "\n",
        "seed = random.randint(0, 2**32 - 1)\n",
        "print(\"Random seed: {}\".format(seed))\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "#torch.backends.cudnn.deterministic=True # warning, slower\n",
        "\n",
        "# create pytorch data object with dfp dataset\n",
        "data = TurbDataset(prop, shuffle=1)\n",
        "trainLoader = DataLoader(data, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "print(\"Training batches: {}\".format(len(trainLoader)))\n",
        "dataValidation = dataset.ValiDataset(data)\n",
        "valiLoader = DataLoader(dataValidation, batch_size=batch_size, shuffle=False, drop_last=True) \n",
        "print(\"Validation batches: {}\".format(len(valiLoader)))\n",
        "\n",
        "# setup training\n",
        "epochs = int(iterations/len(trainLoader) + 0.5)\n",
        "netG = TurbNetG(channelExponent=expo, dropout=dropout)\n",
        "print(netG) # print full net\n",
        "model_parameters = filter(lambda p: p.requires_grad, netG.parameters())\n",
        "params = sum([np.prod(p.size()) for p in model_parameters])\n",
        "print(\"Initialized TurbNet with {} trainable params \".format(params))\n",
        "\n",
        "netG.apply(weights_init)\n",
        "if len(doLoad)>0:\n",
        "    netG.load_state_dict(torch.load(doLoad))\n",
        "    print(\"Loaded model \"+doLoad)\n",
        "netG.cuda()\n",
        "\n",
        "criterionL1 = nn.L1Loss()\n",
        "criterionL1.cuda()\n",
        "\n",
        "optimizerG = optim.Adam(netG.parameters(), lr=lrG, betas=(0.5, 0.999), weight_decay=0.0)\n",
        "\n",
        "targets = Variable(torch.FloatTensor(batch_size, 3, 128, 128))\n",
        "inputs  = Variable(torch.FloatTensor(batch_size, 3, 128, 128))\n",
        "targets = targets.cuda()\n",
        "inputs  = inputs.cuda()\n",
        "\n",
        "##########################\n",
        "\n",
        "columns = ['epoch','id', 'Pressure','V_x','V_y']\n",
        "test_table= wandb.Table(columns=columns)\n",
        "start_time = time.time()\n",
        "for epoch in range(epochs):\n",
        "    print(\"Starting epoch {} / {}\".format((epoch+1),epochs))\n",
        "    \n",
        "\n",
        "    \"Training loop\"\n",
        "    netG.train()\n",
        "    L1_accum = 0.0\n",
        "\n",
        "    for i, traindata in enumerate(trainLoader, 0):\n",
        "        inputs_cpu, targets_cpu = traindata\n",
        "        targets_cpu, inputs_cpu = targets_cpu.float().cuda(), inputs_cpu.float().cuda()\n",
        "        inputs.resize_as_(inputs_cpu).copy_(inputs_cpu)\n",
        "        targets.resize_as_(targets_cpu).copy_(targets_cpu)\n",
        "\n",
        "        # compute LR decay\n",
        "        if decayLr:\n",
        "            currLr = utils.computeLR(epoch, epochs, lrG*0.1, lrG)\n",
        "            if currLr < lrG:\n",
        "                for g in optimizerG.param_groups:\n",
        "                    g['lr'] = currLr\n",
        "\n",
        "        netG.zero_grad()\n",
        "        gen_out = netG(inputs)\n",
        "\n",
        "        lossL1 = criterionL1(gen_out, targets)\n",
        "        lossL1.backward()\n",
        "\n",
        "        optimizerG.step()\n",
        "\n",
        "        lossL1viz = lossL1.item()\n",
        "        L1_accum += lossL1viz\n",
        "        \n",
        "        wandb.log({'Train_loss':L1_accum/len(trainLoader), 'Epoch':epoch})\n",
        "\n",
        "        if i==len(trainLoader)-1:\n",
        "            logline = \"Epoch: {}, batch-idx: {}, L1: {}\\n\".format(epoch, i, lossL1viz)\n",
        "            print(logline)\n",
        "\n",
        "\n",
        "    # validation\n",
        "    netG.eval()\n",
        "    L1val_accum = 0.0\n",
        "    test_loss_accum = 0\n",
        "\n",
        "    dataset = TurbDataset(None, mode=TurbDataset.TEST, dataDirTest= r\"/content/gdrive/MyDrive/data/test\")\n",
        "    testLoader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "    # data for graph plotting\n",
        "    L1_accum    /= len(trainLoader)\n",
        "    L1val_accum /= len(valiLoader)\n",
        "    if saveL1:\n",
        "        if epoch==0: \n",
        "            utils.resetLog(prefix + \"L1.txt\"   )\n",
        "            utils.resetLog(prefix + \"L1val.txt\")\n",
        "        utils.log(prefix + \"L1.txt\"   , \"{} \".format(L1_accum), False)\n",
        "        utils.log(prefix + \"L1val.txt\", \"{} \".format(L1val_accum), False)\n",
        "\n",
        "    '''Relative error per channel'''\n",
        "    R_Error         = torch.zeros(size=(1,n_var),device=device)\n",
        "\n",
        "    counter = 0\n",
        "    for i, data in enumerate(testLoader, 0):\n",
        "\n",
        "      counter +=1\n",
        "      inputs_cpu, targets_cpu = data\n",
        "      targets_cpu1, inputs_cpu = targets_cpu.float().cuda(), inputs_cpu.float().cuda()\n",
        "      inputs.resize_as_(inputs_cpu).copy_(inputs_cpu)\n",
        "      targets.resize_as_(targets_cpu1).copy_(targets_cpu)     \n",
        "      #inputs.data.resize_as_(inputs_cpu).copy_(inputs_cpu)\n",
        "      #targets.data.resize_as_(targets_cpu1).copy_(targets_cpu)\n",
        "\n",
        "\n",
        "      outputs = netG(inputs)\n",
        "      outputs_cpu = outputs.data.cpu().numpy()[0]\n",
        "      targets_cpu = targets_cpu1.cpu().numpy()[0] \n",
        "\n",
        "      outputs_gpu_test = outputs.data\n",
        "      targets_gpu_test = targets_cpu1   \n",
        "\n",
        "      loss             = criterionL1(outputs_gpu_test,targets_gpu_test)\n",
        "      test_loss_accum += loss.item()\n",
        "      R_Error         += relative_error(outputs_gpu_test,targets_gpu_test, inputs_cpu[:,2,:,:])\n",
        "\n",
        "      if counter % 30==0:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        images = []\n",
        "\n",
        "        for i in range(n_var):\n",
        "          x        = np.reshape(targets_cpu[i,:,:],(128,128)).T\n",
        "          y        = np.reshape(outputs_cpu[i,:,:],(128,128)).T\n",
        "          img_data = wandb.Image(np.concatenate((x,y),axis=1))\n",
        "          images.append(img_data)\n",
        "\n",
        "        test_table.add_data(epoch,counter,*images)\n",
        "\n",
        "    wandb.log({'Channels':test_table})\n",
        "  \n",
        "\n",
        "    E_P, E_V_x, E_V_y  = R_Error[0]/len(testLoader)\n",
        "\n",
        "    wandb.log({'Test_loss':test_loss_accum/len(testLoader),'Epoch':epoch,\\\n",
        "             'E_P':E_P,'E_V_x':E_V_x,'E_V_y':E_V_y})\n",
        "  \n",
        "\n",
        "end_time = time.time()\n",
        "total_time = end_time - start_time\n",
        "print(\"Time: \", total_time)\n",
        "\n",
        "torch.save(netG.state_dict(), prefix + \"ModelGNew6.pth\" )"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}